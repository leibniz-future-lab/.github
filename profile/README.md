## Leibniz AI Lab

In the International Future Lab for Artificial Intelligence ([Leibniz AI Lab](https://leibniz-ai-lab.de/)) in Hannover, excellent international researchers as well as renowned colleagues from L3S Research Center, Leibniz University, Hannover Medical School and European partner institutes have been researching new topics in artificial intelligence and developing intelligent solutions for personalised medicine.

* [SelfDistill-SER](https://github.com/leibniz-future-lab/SelfDistill-SER): **Fast yet effective speech emotion recognition with self-distillation.**
<br> In this paper, self-distillation was applied to produce a fast and effective SER model, by simultaneously fine-tuning wav2vec 2.0 and training its shallower versions.
* [PrototypeSound](https://github.com/leibniz-future-lab/PrototypeSound): **Prototype Learning for Interpretable Respiratory Sound Analysis.**
<br> The prototype learning framework aims to generate prototypes of audio singnals for a respiratory sound classification task (normal/crackle/wheeze/both).
* [HypercomplexECG](https://github.com/leibniz-future-lab/HypercomplexECG): **Efficient ECG-based Atrial Fibrillation Detection via Parameterised Hypercomplex Neural Networks.** 
<br> We propose lightweight convolutional neural networks  for atrial fibrillation detection based on the recently proposed parameterised hypercomplex neural networks.
* [Knowledge Acquisition](https://github.com/jwallat/knowledge-acquisition): **The Effect of Masking Strategies on Knowledge Retention by Language Model**
<br> In this work, we investigate how different training regimes affect the amount of factual knowledge that language models remember. We test for masking random words, entities, and masking multiple tokens based on point-wise mutual information.
 
* [Knowledge Probing](https://github.com/jwallat/knowledge-probing): **BERTnesia: Investigating the capture and forgetting of knowledge in BERT**
<br> We investigate how much factual knowledge is retained in the individual layers of language models. To do so, we use cloze questions (e.g., The capital of France is ___).
 
* [Probing Search](https://github.com/yolomeus/probing-search): **Probing BERT for Ranking Abilities**
<br> We investigate to what degree LMs for information retrieval encode standard abilities such as lexical or semantic similarity, named entity recognition, and others. We use that information where these models learn such abilities to train more effective models.
 
* [Temporal Blind Spots](https://github.com/jwallat/temporalblindspots): **Temporal Blindspots in Large Language Models**
<br> We investigate to what degree LLMs are able to answer questions about historical events. Further, we explore common errors that occur when doing so.
